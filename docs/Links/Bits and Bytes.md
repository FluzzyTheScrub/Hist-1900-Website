#TheInformationParadox #Codes #OriginsOfTheBinaryCode #Link 

Within [[@The Information Paradox]], the basis of information begins with the concept of bit. The article describes:
>Shannon introduced the term bit (short for binary digit) to describe the basic unit of information. Information can can be represented as patterns of bits, a process called digitalization- literally, the conversion of analog information into digits.
>[[@The Information Paradox]] Pg 470

This concept directly relates to the article [[@Origins of the Binary Code]], as this concept is the core basics of Binary code. Within Binary code, each sequence of 1's and 0's representing a different bit of information.

The reason why Binary uses a sequence of numbers to represent a single bit, rather than only a few, is additionally explained in the article as it explains:

[[@The Information Paradox]] Pg 473
![[Pasted image 20230310174803.png]]

>Shannon defined the amount of information contained in a message as the number of yes-no questions needed to select the message from the source. The addtitional questions reduce uncertainty about which message us sent.
>[[@The Information Paradox]] Pg 473

Essentially, the more yes/no questions (or 1's and 0's) you have, the more potential information can be represented.

Ex: 2 questions could only display 2^2 or 4 potential final outcomes, while 4 questions would allow you to have 4^4 or 256 possible final meanings. 

256 additionally is the bit limit for 4-bit computing, since each bit is represented by 4 slots for 1 or 0.

2023-03-10





